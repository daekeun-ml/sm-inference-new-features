{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2bebec2",
   "metadata": {},
   "source": [
    "# Module 1. Check Inference Results & Local Mode Deployment\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "본 핸즈온은 AWS AIML Blog의 내용을 기반으로 MNIST 예제 대신 좀 더 실용적인 한국어 자연어 처리 예시를 다루며, 총 3종류(Sentiment Classification, KorSTS, KoBART)의 자연어 처리 모델을 SageMaker 다중 컨테이너 엔드포인트(Multi-container endpoint)로 배포하는 법을 익혀 봅니다.\n",
    "\n",
    "이미 SageMaker 기본 개념(로컬 모드, 호스팅 엔드포인트)과 자연어 처리 & Huggingface을 다뤄 보신 분들은 이 섹션을 건너 뛰고 다음 노트북으로 진행하셔도 됩니다.\n",
    "\n",
    "### References\n",
    "- AWS AIML Blog: https://aws.amazon.com/ko/blogs/machine-learning/deploy-multiple-serving-containers-on-a-single-instance-using-amazon-sagemaker-multi-container-endpoints/\n",
    "- Developer Guide: https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7d4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (1.19.2)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 48.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Collecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (3.7.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.3.1-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 68.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Installing collected packages: filelock, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.3.1 huggingface-hub-0.0.19 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372f5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import boto3\n",
    "import sagemaker\n",
    "import datetime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from src.utils import print_outputs, prepare_model_artifact, NLPPredictor \n",
    "\n",
    "role = get_execution_role()\n",
    "boto_session = boto3.session.Session()\n",
    "sm_session = sagemaker.session.Session()\n",
    "sm_client = boto_session.client(\"sagemaker\")\n",
    "sm_runtime = boto_session.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186c757",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Check Inference Results & Debugging\n",
    "---\n",
    "\n",
    "로컬 엔드포인트나 호스팅 엔드포인트 배포 전, 로컬 환경 상에서 직접 추론을 수행하여 결과를 확인합니다. 참고로, SageMaker에서 TensorFlow를 제외한 머신 러닝 프레임워크 추론 컨테이너는 아래의 인터페이스를 사용합니다.\n",
    "\n",
    "#### Option 1.\n",
    "- `model_fn(model_dir)`: 네트워크 아키텍처를 정의하고 S3의 model_dir에 저장된 모델 아티팩트를 로드합니다.\n",
    "- `input_fn(request_body, content_type)`: 입력 데이터를 전처리합니다. (예: request_body로 전송된 bytearray 배열을 PIL.Image로 변환 수 cropping, resizing, normalization등의 전처리 수행). content_type은 입력 데이터 종류에 따라 다양하게 처리 가능합니다. (예: application/x-npy, application/json, application/csv 등)\n",
    "- `predict_fn(input_object, model)`: input_fn을 통해 들어온 데이터에 대해 추론을 수행합니다.\n",
    "- `output_fn(prediction, accept_type)`: predict_fn에서 받은 추론 결과를 추가 변환을 거쳐 프론트 엔드로 전송합니다.\n",
    "\n",
    "#### Option 2.\n",
    "- `model_fn(model_dir)`: 네트워크 아키텍처를 정의하고 S3의 model_dir에 저장된 모델 아티팩트를 로드합니다.\n",
    "- `transform_fn(model, request_body, content_type, accept_type)`: input_fn(), predict_fn(), output_fn()을 transform_fn()으로 통합할 수 있습니다.\n",
    "\n",
    "본 핸즈온은\n",
    "Option 예시로 \n",
    "\n",
    "모델, 배포에 초점을 맞추기 위해 Huggingface에 등록된 `KoELECTRA-Small-v3` 모델을 기반으로 네이버 영화 리뷰 데이터셋과 KorSTS (Korean Semantic Textual Similarity) 데이터셋으로 파인 튜닝하였습니다. 파인 튜닝은 온프레미스나 Huggingface on SageMaker로 쉽게 수행 가능합니다. \n",
    "\n",
    "- KoELECTRA: https://github.com/monologg/KoELECTRA\n",
    "- Huggingface on Amazon SageMaker: https://huggingface.co/docs/sagemaker/main\n",
    "\n",
    "\n",
    "### Model A: Sentiment Classification\n",
    "\n",
    "네이버 영화 리뷰 데이터의 긍정/부정 판별 예시입니다. \n",
    "- Naver sentiment movie corpus: https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08dca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/inference_nsmc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daeca50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a75ce40df842788f8720d756e90918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/61.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70761ff3b46749a2b0337c1f97198356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9822da6667ed454fb74d734d29e89dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/257k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{inference_nsmc.py:40} INFO - input text: 이 영화는 최고의 영화입니다\n",
      "[{inference_nsmc.py:40} INFO - input text: 최악이에요. 배우의 연기력도 좋지 않고 내용도 너무 허접합니다\n",
      "[{inference_nsmc.py:70} INFO - predicted_class: Pos\n",
      "[{inference_nsmc.py:77} INFO - jsonline: {\"predicted_label\": \"Pos\", \"score\": 0.9619030952453613}\n",
      "[{inference_nsmc.py:70} INFO - predicted_class: Neg\n",
      "[{inference_nsmc.py:77} INFO - jsonline: {\"predicted_label\": \"Neg\", \"score\": 0.9994170665740967}\n",
      "{\"predicted_label\": \"Pos\", \"score\": 0.9619030952453613}\n",
      "{\"predicted_label\": \"Neg\", \"score\": 0.9994170665740967}\n"
     ]
    }
   ],
   "source": [
    "from src.inference_nsmc import model_fn, input_fn, predict_fn, output_fn\n",
    "modelA_path = 'model-nsmc'\n",
    "\n",
    "with open('samples/nsmc.txt', mode='rb') as file:\n",
    "    modelA_input_data = file.read()\n",
    "\n",
    "modelA = model_fn(modelA_path)\n",
    "transformed_inputs = input_fn(modelA_input_data)\n",
    "predicted_classes_jsonlines = predict_fn(transformed_inputs, modelA)\n",
    "modelA_outputs = output_fn(predicted_classes_jsonlines)\n",
    "print(modelA_outputs[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fb2ea",
   "metadata": {},
   "source": [
    "### Model B: Semantic Textual Similarity (STS)\n",
    "\n",
    "두 문장간의 유사도를 정량화하는 예시입니다.\n",
    "- KorNLI and KorSTS: https://github.com/kakaobrain/KorNLUDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/inference_korsts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6070d006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{inference_korsts.py:40} INFO - input text: ['맛있는 라면을 먹고 싶어요', '후루룩 쩝쩝 후루룩 쩝쩝 맛좋은 라면']\n",
      "[{inference_korsts.py:40} INFO - input text: ['뽀로로는 내친구', '머신러닝은 러닝머신이 아닙니다.']\n",
      "[{inference_korsts.py:67} INFO - jsonline: {\"score\": 4.786738872528076}\n",
      "[{inference_korsts.py:67} INFO - jsonline: {\"score\": 0.23190681636333466}\n",
      "{\"score\": 4.786738872528076}\n",
      "{\"score\": 0.23190681636333466}\n"
     ]
    }
   ],
   "source": [
    "from src.inference_korsts import model_fn, input_fn, predict_fn, output_fn\n",
    "modelB_path = 'model-korsts'\n",
    "\n",
    "with open('samples/korsts.txt', mode='rb') as file:\n",
    "    modelB_input_data = file.read()    \n",
    "    \n",
    "modelB = model_fn(modelB_path)\n",
    "transformed_inputs = input_fn(modelB_input_data)\n",
    "predicted_classes_jsonlines = predict_fn(transformed_inputs, modelB)\n",
    "modelB_outputs = output_fn(predicted_classes_jsonlines)\n",
    "print(modelB_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff9e99",
   "metadata": {},
   "source": [
    "### Model C: KoBART (Korean Bidirectional and Auto-Regressive Transformers)\n",
    "\n",
    "문서 내용(예: 뉴스 기사)을 요약하는 예시입니다.\n",
    "\n",
    "- KoBART: https://github.com/SKT-AI/KoBART\n",
    "- KoBART Summarization: https://github.com/seujung/KoBART-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/inference_kobart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3cefd",
   "metadata": {},
   "source": [
    "S3로 모델 아티팩트를 복사하는 대신 Huggingface에 등록된 모델을 그대로 사용합니다. model.pth는 0바이트의 빈 파일이며, 추론을 수행하기 위한 소스 코드들만 아카이빙됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3650642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d4bbff12fb4287b29ea681239fe1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5973577c90b24d9bac0973ddae90da03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/302 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d947e7baa380445da3abf957059aa916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7553a279cb44309219898b8c10df19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b2df3a2c59485d85981332c8f74d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{inference_kobart.py:36} INFO - input text: AWS가 10일 AWS 리인벤트를 통해 머신러닝 서비스인 아마존 세이지메이커(Amazon SageMaker)의 9가지 새로운 기능을 발표했다. 아마존 세이지메이커 데이터 랭글러(Amazon SageMaker Data Wrangler)를 비롯해 아마존 세이지메이커 피처 스토어 등 다양한 기술들이 속속 베일을 벗었다. 스와미 시바수브라마니안(Swami Sivasubramanian), AWS 아마존 머신러닝 부사장은 '수십만 명의 일반 개발자와 데이터 과학자가 업계 최고의 머신러닝 서비스인 아마존 세이지메이커를 활용해 맞춤형 머신러닝 모델 제작, 훈련 및 배치에 대한 장벽을 제거했다'면서 '개발자가 더 나은 가시성, 설명 가능성 및 자동화를 대규모로 구현하는 맞춤형 머신러닝 모델을 준비, 제작, 훈련, 설명, 검사, 모니터링, 디버그 및 실행하기 위한 엔드투엔드 머신러닝 파이프라인을 더 쉽게 구축할 수 있도록 지원한다'고 말했다.\n",
      "[{inference_kobart.py:52} INFO - summary_outputs: AWS가 10일 AWS 리인벤트를 통해 머신러닝 서비스인 아마존 세이지메이커 데이터 랭글러를 비롯한 아마존 세이지메이커 피처 스토어 등 9가지 새로운 기능을 발표하며 개발자가 더 나은 가시성, 설명 가능성 및 자동화를 대규모로 구현하는 맞춤형 머신러닝 모델을 준비, 제작, 훈련, 설명, 검사, 모니터링, 디버그 및 실행하기 위한 엔드투엔드 머신러닝 파이프라인을 더 쉽게 구축할 수 있도록 지원한다고 말했다.\n",
      "[{inference_kobart.py:36} INFO - input text: 대한항공은 이와 같은 필요성에 따라 AWS와 AWS의 국내 파트너사인 LG CNS와 함께 기존 사내 데이터 센터에서 운영했던 데이터와 네트워크, 보안 시스템을 비롯한 각종 IT시스템을 단계적으로 AWS의 클라우드로 이전해 효율성을 높이고 IT 관리를 단순화했다. 대한항공은 이번 전사 IT시스템의 클라우드 이전 완료에 따라 데이터 분석 능력, 머신러닝등 아마존웹서비스가 갖고 있는 클라우드 기능을 바탕으로 경영 프로세스 혁신, 여객서비스 강화, 예약·발권 시스템 편의성 증대, 기상예측 정확도 제고 등을 추진해 나간다. 대한항공은 먼저 ‘클라우드 머신러닝 관리 서비스’를 도입한다. 이는 머신러닝 모델의 구축, 학습, 적용을 모두 하나의 환경에서 관리할 수 있도록 해주는 서비스로 정확한 수요 및 통계 예측을 지원함으로써 보다 나은 고객 서비스를 제공할 수 있게 한다. 특히 악천후로 인한 항공기 지연 예상시간, 항공기 정비 소요시간 예측 등을 토대로 고객들에게 적절한 시점에 필요한 조치를 할 수 있을 것으로 기대된다. 또 AWS 클라우드로 구축된 고객 데이터 플랫폼에서 고객별 특성에 따른 고유 디지털 식별 정보가 부여돼, 맞춤형 고객 서비스 제공도 가능해질 것으로 보고있다. 다시 말해, 그 동안 고객이 대한항공으로부터 제공 받은 서비스를 포함한 각종 정보들을 종합적으로 분석해 고객 니즈에 맞는 맞춤 서비스를 추천하는 기능도 제공된다는 것이다.\n",
      "[{inference_kobart.py:52} INFO - summary_outputs: 대한항공은 사내 사내 데이터 센터에서 운영했던 데이터와 네트워크, 보안 시스템을 비롯한 각종 IT시스템을 단계적으로 AWS의 클라우드로 이전해 효율성을 높이고 IT 관리를 단순화했으며 클라우드 머신러닝 관리 서비스’를 도입하여 악천후로 인한 항공기 지연 예상시간, 정비 소요시간 예측 등을 토대로 적절한 시점에 필요한 조치를 할 수 있을 것으로 기대된다.\n"
     ]
    }
   ],
   "source": [
    "from src.inference_kobart import model_fn, transform_fn\n",
    "modelC_path = 'model-kobart'\n",
    "f = open(f\"{modelC_path}/model.pth\", 'w')\n",
    "f.close()\n",
    "\n",
    "with open('samples/kobart.txt', mode='rb') as file:\n",
    "    modelC_input_data = file.read()\n",
    "\n",
    "modelC = model_fn('./')\n",
    "outputs = transform_fn(modelC, modelC_input_data)\n",
    "\n",
    "with open('samples/kobart.txt', mode='rb') as file:\n",
    "    modelC_input_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0997d8",
   "metadata": {},
   "source": [
    "결괏값들을 확인했다면 로컬 모드로 빠르게 배포하여 테스트하는 것을 권장드립니다. 단, SageMaker Studio는 로컬 모드를 지원하지 않기 때문에 아래 섹션은 SageMaker에서 실행해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c721510",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. (SageMaker Only) Local Mode Deployment for Model A\n",
    "---\n",
    "\n",
    "### Deploy Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ab4182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archived modelA.tar.gz\n",
      "[{session.py:2655} INFO - Creating model with name: pytorch-inference-2021-10-16-12-17-24-959\n",
      "[{session.py:3012} INFO - Creating endpoint with name pytorch-inference-2021-10-16-12-17-24-962\n",
      "[{image.py:269} INFO - serving\n",
      "[{image.py:272} INFO - creating hosting dir in /tmp/tmp81gdbind\n",
      "[{image.py:1007} INFO - No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "[{image.py:681} INFO - docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-tmxkl:\n",
      "    command: serve\n",
      "    container_name: p0pe22flyp-algo-1-tmxkl\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.7.1-cpu-py3\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-tmxkl\n",
      "    ports:\n",
      "    - 8080:8080\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpezmngm0g:/opt/ml/model\n",
      "version: '2.3'\n",
      "\n",
      "[{image.py:704} INFO - docker command: docker-compose -f /tmp/tmp81gdbind/docker-compose.yaml up --build --abort-on-container-exit\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 5\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3e35f38358>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3e35f38c18>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3e35f38ac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 10\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "Attaching to p0pe22flyp-algo-1-tmxkl\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Collecting transformers==4.11.3\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 25.3 MB/s eta 0:00:01\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m \u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (5.4.1)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Collecting filelock\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading filelock-3.3.1-py3-none-any.whl (9.7 kB)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Collecting sacremoses\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 40.2 MB/s eta 0:00:01\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m \u001b[?25hCollecting importlib-metadata\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 48.4 MB/s eta 0:00:01\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m \u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 15\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading regex-2021.10.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "\u001b[K     |████████████████████████████████| 749 kB 38.7 MB/s eta 0:00:01\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m \u001b[?25hCollecting huggingface-hub>=0.0.17\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m \u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (4.59.0)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (3.10.0.0)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Collecting packaging>=20.0\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading packaging-21.0-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m \u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.4.7)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Collecting zipp>=0.5\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2020.12.5)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Collecting click\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m \u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Installing collected packages: zipp, importlib-metadata, regex, packaging, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m   Attempting uninstall: packaging\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m     Found existing installation: packaging 20.4\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m     Uninstalling packaging-20.4:\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m       Successfully uninstalled packaging-20.4\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Successfully installed click-8.0.3 filelock-3.3.1 huggingface-hub-0.0.19 importlib-metadata-4.8.1 packaging-21.0 regex-2021.10.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3 zipp-3.6.0\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m \u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 20\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:42,242 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Torchserve version: 0.3.1\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m TS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Current directory: /\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Number of CPUs: 2\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Max heap size: 988 M\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Python executable: /opt/conda/bin/python3.6\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Log dir: /logs\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Netty threads: 0\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Netty client threads: 0\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Default workers per model: 2\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Enable metrics API: true\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:42,297 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,060 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag bd68f39cd6bc4df4a489c662134638fa\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,118 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,142 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,332 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,332 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,347 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,412 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,414 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]51\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,414 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,415 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,423 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,424 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]50\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,425 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,425 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,433 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,436 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,474 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:44,476 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m Model server started.\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:45,059 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:4892e8d12952,timestamp:1634386665\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:45,066 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:37.64773941040039|#Level:Host|#hostname:4892e8d12952,timestamp:1634386665\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:45,074 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:8.801937103271484|#Level:Host|#hostname:4892e8d12952,timestamp:1634386665\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:45,075 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:18.9|#Level:Host|#hostname:4892e8d12952,timestamp:1634386665\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:45,076 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1444.81640625|#Level:Host|#hostname:4892e8d12952,timestamp:1634386665\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:45,077 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:2268.7734375|#Level:Host|#hostname:4892e8d12952,timestamp:1634386665\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:45,078 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:63.4|#Level:Host|#hostname:4892e8d12952,timestamp:1634386665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,172 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,176 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/61.0 [00:00<?, ?B/s]\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,287 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 61.0/61.0 [00:00<00:00, 15.5kB/s]\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,287 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,289 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/467 [00:00<?, ?B/s]\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,418 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 467/467 [00:00<00:00, 180kB/s]\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,420 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,429 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/257k [00:00<?, ?B/s]\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 25\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,772 [INFO ] pool-1-thread-3 ACCESS_LOG - /172.18.0.1:53780 \"GET /ping HTTP/1.1\" 200 19\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:46,772 [INFO ] pool-1-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:4892e8d12952,timestamp:null\n",
      "!"
     ]
    }
   ],
   "source": [
    "modelA_artifact_name = 'modelA.tar.gz'\n",
    "prepare_model_artifact(modelA_path, model_artifact_name=modelA_artifact_name)\n",
    "local_model_path = f'file://{os.getcwd()}/{modelA_artifact_name}'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=local_model_path,\n",
    "    role=role,\n",
    "    entry_point='inference_nsmc.py', \n",
    "    source_dir='src',\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py3',\n",
    "    predictor_cls=NLPPredictor,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480c651",
   "metadata": {},
   "source": [
    "### Invoke using SageMaker Python SDK\n",
    "SageMaker SDK `predict()` 메서드로 간단하게 추론을 실행할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7700fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,046 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - input text: 이 영화는 최고의 영화입니다\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,048 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - input text: 최악이에요. 배우의 연기력도 좋지 않고 내용도 너무 허접합니다\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,229 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - predicted_class: Pos\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,229 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - jsonline: {\"predicted_label\": \"Pos\", \"score\": 0.9619030952453613}\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,311 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - predicted_class: Neg\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,312 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - jsonline: {\"predicted_label\": \"Neg\", \"score\": 0.9994170665740967}\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,314 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 269\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,314 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:267.48|#ModelName:model,Level:Model|#hostname:4892e8d12952,requestID:c610e5bb-cd9a-42e4-ac73-bd59b0f7c50c,timestamp:1634386676\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,315 [INFO ] W-9001-model_1 ACCESS_LOG - /172.18.0.1:54162 \"POST /invocations HTTP/1.1\" 200 282\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,316 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:4892e8d12952,timestamp:null\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,317 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:4892e8d12952,timestamp:null\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:17:56,317 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:4892e8d12952,timestamp:null\n"
     ]
    }
   ],
   "source": [
    "inputs = [{\"text\": [\"이 영화는 최고의 영화입니다\"]}, \n",
    "          {\"text\": [\"최악이에요. 배우의 연기력도 좋지 않고 내용도 너무 허접합니다\"]}]\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202e1d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_label': 'Pos', 'score': 0.9619030952453613}\n",
      "{'predicted_label': 'Neg', 'score': 0.9994170665740967}\n"
     ]
    }
   ],
   "source": [
    "for c in predicted_classes:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb5424",
   "metadata": {},
   "source": [
    "### Invoke using Boto3 API\n",
    "이번에는 boto3의 `invoke_endpoint()` 메서드로 추론을 수행해 보겠습니다.\n",
    "Boto3는 서비스 레벨의 low-level SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 high-level SDK인 SageMaker SDK와 달리 SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e43668e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,724 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - input text: 이 영화는 최고의 영화입니다\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,725 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - input text: 최악이에요. 배우의 연기력도 좋지 않고 내용도 너무 허접합니다\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,809 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - predicted_class: Pos\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,810 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - jsonline: {\"predicted_label\": \"Pos\", \"score\": 0.9619030952453613}\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,901 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - predicted_class: Neg\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,902 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - jsonline: {\"predicted_label\": \"Neg\", \"score\": 0.9994170665740967}\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,904 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 179\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,905 [INFO ] W-9000-model_1 ACCESS_LOG - /172.18.0.1:54442 \"POST /invocations HTTP/1.1\" 200 187\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,904 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:180.81|#ModelName:model,Level:Model|#hostname:4892e8d12952,requestID:f02b1caa-0b92-4568-bc54-7b03925f3ad8,timestamp:1634386682\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,905 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:4892e8d12952,timestamp:null\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,907 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:4892e8d12952,timestamp:null\n",
      "\u001b[36mp0pe22flyp-algo-1-tmxkl |\u001b[0m 2021-10-16 12:18:02,908 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:4892e8d12952,timestamp:null\n"
     ]
    }
   ],
   "source": [
    "local_sm_runtime = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = model.endpoint_name\n",
    "\n",
    "response = local_sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/jsonlines',\n",
    "    Accept='application/jsonlines',\n",
    "    Body=modelA_input_data\n",
    "    )\n",
    "outputs = response['Body'].read().decode()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3abd9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_label': 'Pos', 'score': 0.9619030952453613}\n",
      "{'predicted_label': 'Neg', 'score': 0.9994170665740967}\n"
     ]
    }
   ],
   "source": [
    "print_outputs(outputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609afd7",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. SageMaker SDK에서는 delete_endpoint() 메소드로 간단히 삭제할 수 있습니다.\n",
    "참고로, 노트북 인스턴스에서 추론 컨테이너를 배포했기 때문에 엔드포인트를 띄워 놓아도 별도로 추가 요금이 과금되지는 않습니다.\n",
    "\n",
    "로컬 엔드포인트는 도커 컨테이너이기 때문에 `docker rm $(docker ps -a -q)` 으로도 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15ca2466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{session.py:3072} INFO - Deleting endpoint configuration with name: pytorch-inference-2021-10-16-12-17-24-962\n",
      "[{session.py:3062} INFO - Deleting endpoint with name: pytorch-inference-2021-10-16-12-17-24-962\n",
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d72302",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. (SageMaker Only) Local Mode Deployment for Model B\n",
    "---\n",
    "\n",
    "### Deploy Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cca9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB_artifact_name = 'modelB.tar.gz'\n",
    "prepare_model_artifact(modelB_path, model_artifact_name=modelB_artifact_name)\n",
    "local_model_path = f'file://{os.getcwd()}/{modelB_artifact_name}'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=local_model_path,\n",
    "    role=role,\n",
    "    entry_point='inference_korsts.py', \n",
    "    source_dir='src',\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py3',\n",
    "    predictor_cls=NLPPredictor,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed424a",
   "metadata": {},
   "source": [
    "### Invoke using SageMaker Python SDK\n",
    "SageMaker SDK `predict()` 메서드로 간단하게 추론을 실행할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{\"text\": [\"맛있는 라면을 먹고 싶어요\", \"후루룩 쩝쩝 후루룩 쩝쩝 맛좋은 라면\"]}, \n",
    "          {\"text\": [\"뽀로로는 내친구\", \"머신러닝은 러닝머신이 아닙니다.\"]}]\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in predicted_classes:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a287df1",
   "metadata": {},
   "source": [
    "### Invoke using Boto3 API\n",
    "이번에는 boto3의 `invoke_endpoint()` 메서드로 추론을 수행해 보겠습니다.\n",
    "Boto3는 서비스 레벨의 low-level SDK로, ML 실험에 초점을 맞춰 일부 기능들이 추상화된 high-level SDK인 SageMaker SDK와 달리 SageMaker API를 완벽하게 제어할 수 있습으며, 프로덕션 및 자동화 작업에 적합합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sm_runtime = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = model.endpoint_name\n",
    "\n",
    "response = local_sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/jsonlines',\n",
    "    Accept='application/jsonlines',\n",
    "    Body=modelB_input_data\n",
    "    )\n",
    "outputs = response['Body'].read().decode()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514af9fd",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. SageMaker SDK에서는 delete_endpoint() 메소드로 간단히 삭제할 수 있습니다.\n",
    "참고로, 노트북 인스턴스에서 추론 컨테이너를 배포했기 때문에 엔드포인트를 띄워 놓아도 별도로 추가 요금이 과금되지는 않습니다.\n",
    "\n",
    "로컬 엔드포인트는 도커 컨테이너이기 때문에 `docker rm $(docker ps -a -q)` 으로도 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e5954",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. (SageMaker Only) Local Mode Deployment for Model C\n",
    "---\n",
    "\n",
    "### Deploy Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9e8485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archived modelC.tar.gz\n",
      "[{session.py:2655} INFO - Creating model with name: pytorch-inference-2021-10-16-12-37-07-597\n",
      "[{session.py:3012} INFO - Creating endpoint with name pytorch-inference-2021-10-16-12-37-07-599\n",
      "[{image.py:269} INFO - serving\n",
      "[{image.py:272} INFO - creating hosting dir in /tmp/tmpia6u1y2i\n",
      "[{image.py:1090} INFO - docker command: docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.7.1-cpu-py3\n",
      "[{image.py:1093} INFO - image pulled: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.7.1-cpu-py3\n",
      "[{image.py:1007} INFO - No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "[{image.py:681} INFO - docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-avtfx:\n",
      "    command: serve\n",
      "    container_name: 5pyy29obw3-algo-1-avtfx\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.7.1-cpu-py3\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-avtfx\n",
      "    ports:\n",
      "    - 8080:8080\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmp8qlfkaw6:/opt/ml/model\n",
      "version: '2.3'\n",
      "\n",
      "[{image.py:704} INFO - docker command: docker-compose -f /tmp/tmpia6u1y2i/docker-compose.yaml up --build --abort-on-container-exit\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 5\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4e0e66fe48>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4e0e66f588>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4e0e66f320>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 10\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "Attaching to 5pyy29obw3-algo-1-avtfx\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting transformers==4.11.3\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 36.3 MB/s eta 0:00:01\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m \u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 43.3 MB/s eta 0:00:01\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m \u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (4.59.0)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting huggingface-hub>=0.0.17\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 5.0 MB/s  eta 0:00:01\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m \u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (5.4.1)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting sacremoses\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 29.7 MB/s eta 0:00:01\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m \u001b[?25hRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting filelock\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading filelock-3.3.1-py3-none-any.whl (9.7 kB)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading regex-2021.10.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "\u001b[K     |████████████████████████████████| 749 kB 42.0 MB/s eta 0:00:01\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m \u001b[?25hCollecting importlib-metadata\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (3.10.0.0)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting packaging>=20.0\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading packaging-21.0-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m \u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.4.7)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting zipp>=0.5\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2020.12.5)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Collecting click\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 8.2 MB/s  eta 0:00:01\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m \u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.11.3->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 15\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Installing collected packages: zipp, importlib-metadata, regex, packaging, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m   Attempting uninstall: packaging\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m     Found existing installation: packaging 20.4\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m     Uninstalling packaging-20.4:\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m       Successfully uninstalled packaging-20.4\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Successfully installed click-8.0.3 filelock-3.3.1 huggingface-hub-0.0.19 importlib-metadata-4.8.1 packaging-21.0 regex-2021.10.8 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3 zipp-3.6.0\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m \u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 20\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "[{connectionpool.py:781} WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))': /ping\n",
      "[{entities.py:619} INFO - Container still not up, got: -1\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:19,981 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Torchserve version: 0.3.1\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m TS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Current directory: /\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Number of CPUs: 2\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Max heap size: 988 M\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Python executable: /opt/conda/bin/python3.6\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Log dir: /logs\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Netty threads: 0\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Netty client threads: 0\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Default workers per model: 2\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Enable metrics API: true\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:20,194 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:20,271 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 5847b1676795495aa61bfc5f9d82d89e\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:20,334 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:20,387 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,530 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,532 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,533 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,713 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,717 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,727 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]51\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,729 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,729 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,730 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]50\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,732 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,732 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,801 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:21,818 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:22,037 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:22,060 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m Model server started.\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:24,742 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:5315c6100cc3,timestamp:1634387964\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:24,761 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:14.386638641357422|#Level:Host|#hostname:5315c6100cc3,timestamp:1634387964\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:24,762 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:93.72736358642578|#Level:Host|#hostname:5315c6100cc3,timestamp:1634387964\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:24,763 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:86.7|#Level:Host|#hostname:5315c6100cc3,timestamp:1634387964\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:24,782 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1569.84765625|#Level:Host|#hostname:5315c6100cc3,timestamp:1634387964\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:24,783 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:2119.0078125|#Level:Host|#hostname:5315c6100cc3,timestamp:1634387964\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:24,783 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:60.2|#Level:Host|#hostname:5315c6100cc3,timestamp:1634387964\n",
      "[{entities.py:616} INFO - Checking if serving container is up, attempt: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:26,081 [INFO ] pool-1-thread-3 ACCESS_LOG - /172.18.0.1:58886 \"GET /ping HTTP/1.1\" 200 203\n",
      "!\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:26,159 [INFO ] pool-1-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:5315c6100cc3,timestamp:null\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:31,933 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:31,971 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:32,320 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 239/239 [00:00<00:00, 216kB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:32,320 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:32,339 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/302 [00:00<?, ?B/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:32,639 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 302/302 [00:00<00:00, 156kB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:32,639 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:32,733 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/666k [00:00<?, ?B/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:32,774 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  95%|█████████▍| 632k/666k [00:00<00:00, 6.47MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,138 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 666k/666k [00:00<00:00, 4.83MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,139 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,141 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,322 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 1.42k/1.42k [00:00<00:00, 1.30MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,323 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,435 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,540 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 731k/473M [00:00<01:14, 6.67MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,640 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 2.23M/473M [00:00<00:42, 11.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,747 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   1%|          | 3.72M/473M [00:00<00:37, 13.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,847 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   1%|          | 5.19M/473M [00:00<00:35, 13.8MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:34,963 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   1%|▏         | 6.53M/473M [00:00<00:35, 13.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,073 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   2%|▏         | 7.86M/473M [00:00<00:36, 13.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,210 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   2%|▏         | 9.13M/473M [00:00<00:37, 12.8MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,310 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   2%|▏         | 10.4M/473M [00:00<00:41, 11.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,424 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   2%|▏         | 11.5M/473M [00:00<00:41, 11.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,524 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   3%|▎         | 13.0M/473M [00:01<00:38, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,628 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   3%|▎         | 14.2M/473M [00:01<00:39, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,737 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   3%|▎         | 15.3M/473M [00:01<00:39, 12.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,850 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   3%|▎         | 16.5M/473M [00:01<00:40, 11.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:35,950 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   4%|▎         | 17.6M/473M [00:01<00:41, 11.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,059 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   4%|▍         | 19.0M/473M [00:01<00:38, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,170 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   4%|▍         | 20.2M/473M [00:01<00:39, 12.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,283 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   5%|▍         | 21.4M/473M [00:01<00:40, 11.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,393 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   5%|▍         | 22.6M/473M [00:01<00:40, 11.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,504 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   5%|▌         | 24.0M/473M [00:02<00:38, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,604 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   5%|▌         | 25.2M/473M [00:02<00:39, 11.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,714 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   6%|▌         | 26.4M/473M [00:02<00:39, 11.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,831 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   6%|▌         | 27.5M/473M [00:02<00:40, 11.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:36,940 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   6%|▌         | 28.6M/473M [00:02<00:42, 11.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,040 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   6%|▋         | 29.7M/473M [00:02<00:42, 11.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,171 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   7%|▋         | 31.1M/473M [00:02<00:38, 11.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,284 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   7%|▋         | 32.2M/473M [00:02<00:42, 10.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,385 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   7%|▋         | 33.3M/473M [00:02<00:43, 10.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,513 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   7%|▋         | 34.9M/473M [00:03<00:37, 12.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,621 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   8%|▊         | 36.0M/473M [00:03<00:40, 11.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,721 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   8%|▊         | 37.2M/473M [00:03<00:40, 11.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,825 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   8%|▊         | 38.2M/473M [00:03<00:40, 11.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:37,926 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   8%|▊         | 39.4M/473M [00:03<00:40, 11.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,038 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   9%|▊         | 40.4M/473M [00:03<00:40, 11.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,146 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   9%|▉         | 42.0M/473M [00:03<00:36, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,249 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   9%|▉         | 43.2M/473M [00:03<00:37, 12.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,374 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   9%|▉         | 44.6M/473M [00:03<00:35, 12.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,462 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  10%|▉         | 45.8M/473M [00:04<00:36, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,589 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  10%|▉         | 47.0M/473M [00:04<00:36, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,687 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  10%|█         | 48.5M/473M [00:04<00:35, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,800 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  11%|█         | 49.7M/473M [00:04<00:36, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:38,910 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  11%|█         | 51.1M/473M [00:04<00:34, 12.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,019 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  11%|█         | 52.4M/473M [00:04<00:34, 12.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,122 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  11%|█▏        | 53.6M/473M [00:04<00:35, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,238 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  12%|█▏        | 54.9M/473M [00:04<00:35, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,348 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  12%|█▏        | 56.4M/473M [00:04<00:34, 12.8MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,448 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  12%|█▏        | 57.8M/473M [00:05<00:33, 13.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,571 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  13%|█▎        | 59.1M/473M [00:05<00:32, 13.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,685 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  13%|█▎        | 60.6M/473M [00:05<00:33, 12.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,798 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  13%|█▎        | 61.8M/473M [00:05<00:34, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:39,909 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  13%|█▎        | 63.2M/473M [00:05<00:34, 12.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,009 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  14%|█▎        | 64.4M/473M [00:05<00:35, 12.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,109 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  14%|█▍        | 65.6M/473M [00:05<00:34, 12.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,210 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  14%|█▍        | 66.8M/473M [00:05<00:34, 12.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,316 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  14%|█▍        | 68.0M/473M [00:05<00:34, 12.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,435 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  15%|█▍        | 69.2M/473M [00:05<00:34, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,544 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  15%|█▍        | 70.4M/473M [00:06<00:36, 11.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,653 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  15%|█▌        | 71.5M/473M [00:06<00:37, 11.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,754 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  15%|█▌        | 72.7M/473M [00:06<00:37, 11.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,854 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  16%|█▌        | 74.0M/473M [00:06<00:34, 12.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:40,954 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  16%|█▌        | 75.1M/473M [00:06<00:34, 12.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,059 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  16%|█▌        | 76.5M/473M [00:06<00:32, 12.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,171 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  16%|█▋        | 77.7M/473M [00:06<00:33, 12.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,267 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  17%|█▋        | 79.1M/473M [00:06<00:32, 12.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,413 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  17%|█▋        | 80.7M/473M [00:06<00:29, 13.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,524 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  17%|█▋        | 82.0M/473M [00:07<00:33, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,630 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  18%|█▊        | 83.3M/473M [00:07<00:33, 12.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,730 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  18%|█▊        | 84.5M/473M [00:07<00:33, 12.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,874 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  18%|█▊        | 85.8M/473M [00:07<00:32, 12.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:41,977 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  18%|█▊        | 87.0M/473M [00:07<00:36, 11.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,092 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  19%|█▊        | 88.1M/473M [00:07<00:36, 11.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,192 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  19%|█▉        | 89.2M/473M [00:07<00:36, 11.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "modelC_artifact_name = 'modelC.tar.gz'\n",
    "prepare_model_artifact(modelC_path, model_artifact_name=modelC_artifact_name)\n",
    "local_model_path = f'file://{os.getcwd()}/{modelC_artifact_name}'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=local_model_path,\n",
    "    role=role,\n",
    "    entry_point='inference_kobart.py', \n",
    "    source_dir='src',\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py3',\n",
    "    predictor_cls=NLPPredictor,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6f2470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,303 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  19%|█▉        | 90.3M/473M [00:07<00:36, 10.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,420 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  19%|█▉        | 91.7M/473M [00:07<00:33, 11.8MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,537 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  20%|█▉        | 92.8M/473M [00:08<00:35, 11.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,646 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  20%|█▉        | 93.9M/473M [00:08<00:37, 10.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,764 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  20%|██        | 95.0M/473M [00:08<00:37, 10.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,855 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  20%|██        | 96.2M/473M [00:08<00:35, 11.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:42,955 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  21%|██        | 97.3M/473M [00:08<00:35, 11.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,056 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  21%|██        | 98.4M/473M [00:08<00:34, 11.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,167 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  21%|██        | 99.6M/473M [00:08<00:34, 11.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,281 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  21%|██▏       | 101M/473M [00:08<00:33, 11.8MB/s] \n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,389 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 102M/473M [00:08<00:34, 11.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,498 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 103M/473M [00:09<00:34, 11.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,611 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 104M/473M [00:09<00:35, 10.8MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,724 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 105M/473M [00:09<00:34, 11.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,826 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  23%|██▎       | 107M/473M [00:09<00:34, 11.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:43,927 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  23%|██▎       | 108M/473M [00:09<00:34, 11.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,027 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  23%|██▎       | 109M/473M [00:09<00:33, 11.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,137 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  23%|██▎       | 110M/473M [00:09<00:33, 11.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,247 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  24%|██▎       | 111M/473M [00:09<00:32, 11.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,352 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  24%|██▍       | 113M/473M [00:09<00:30, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,482 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  24%|██▍       | 114M/473M [00:10<00:30, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,557 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  24%|██▍       | 115M/473M [00:10<00:30, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,669 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  25%|██▍       | 116M/473M [00:10<00:30, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,769 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  25%|██▍       | 118M/473M [00:10<00:29, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:44,896 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  25%|██▌       | 119M/473M [00:10<00:29, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,001 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  25%|██▌       | 120M/473M [00:10<00:32, 11.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,112 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  26%|██▌       | 121M/473M [00:10<00:31, 11.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,253 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  26%|██▌       | 122M/473M [00:10<00:32, 11.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,358 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  26%|██▌       | 123M/473M [00:10<00:35, 10.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,467 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  26%|██▋       | 125M/473M [00:11<00:35, 10.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,558 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  27%|██▋       | 126M/473M [00:11<00:31, 11.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,693 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  27%|██▋       | 128M/473M [00:11<00:27, 13.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,809 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  27%|██▋       | 129M/473M [00:11<00:28, 12.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:45,930 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  28%|██▊       | 130M/473M [00:11<00:30, 11.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,035 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  28%|██▊       | 131M/473M [00:11<00:32, 11.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,136 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  28%|██▊       | 133M/473M [00:11<00:31, 11.5MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,239 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  28%|██▊       | 134M/473M [00:11<00:30, 11.8MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,351 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  29%|██▊       | 135M/473M [00:11<00:30, 11.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,447 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  29%|██▉       | 136M/473M [00:12<00:30, 11.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,561 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  29%|██▉       | 137M/473M [00:12<00:29, 11.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,656 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  29%|██▉       | 138M/473M [00:12<00:29, 12.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,797 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|██▉       | 140M/473M [00:12<00:29, 11.9MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:46,909 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|██▉       | 141M/473M [00:12<00:31, 10.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,027 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|███       | 142M/473M [00:12<00:33, 10.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,133 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|███       | 143M/473M [00:12<00:33, 10.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,238 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  30%|███       | 144M/473M [00:12<00:33, 10.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,343 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  31%|███       | 145M/473M [00:12<00:32, 10.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,461 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  31%|███       | 147M/473M [00:13<00:29, 11.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,558 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  31%|███       | 148M/473M [00:13<00:29, 11.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,659 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  32%|███▏      | 149M/473M [00:13<00:28, 12.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,759 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  32%|███▏      | 150M/473M [00:13<00:28, 12.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,870 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  32%|███▏      | 151M/473M [00:13<00:27, 12.2MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:47,980 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  32%|███▏      | 153M/473M [00:13<00:27, 12.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,085 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  33%|███▎      | 154M/473M [00:13<00:26, 12.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,202 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  33%|███▎      | 155M/473M [00:13<00:26, 12.6MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,320 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  33%|███▎      | 157M/473M [00:13<00:25, 13.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,421 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  33%|███▎      | 158M/473M [00:13<00:26, 12.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,544 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  34%|███▎      | 159M/473M [00:14<00:27, 12.0MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,677 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  34%|███▍      | 160M/473M [00:14<00:29, 11.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,765 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  34%|███▍      | 162M/473M [00:14<00:26, 12.4MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,865 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  35%|███▍      | 163M/473M [00:14<00:26, 12.3MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:48,982 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  35%|███▍      | 165M/473M [00:14<00:25, 12.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:49,082 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  35%|███▌      | 166M/473M [00:14<00:26, 12.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:49,196 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  35%|███▌      | 167M/473M [00:14<00:26, 12.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:49,305 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  36%|███▌      | 168M/473M [00:14<00:26, 12.1MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:49,427 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  36%|███▌      | 170M/473M [00:14<00:24, 12.7MB/s]\n",
      "\u001b[36m5pyy29obw3-algo-1-avtfx |\u001b[0m 2021-10-16 12:39:49,528 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  36%|███▌      | 171M/473M [00:15<00:25, 12.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5ec2d",
   "metadata": {},
   "source": [
    "### Invoke using Boto3 API\n",
    "**[주의]** BART 모델은 Auto-Regressive 모델로 내부적으로 연산을 많이 수행하여 기본 인스턴스(예: `ml.t2.medium`)를 사용하는 경우, 시간이 상대적으로 오래 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sm_runtime = sagemaker.local.LocalSagemakerRuntimeClient()\n",
    "endpoint_name = model.endpoint_name\n",
    "\n",
    "response = local_sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType='application/jsonlines',\n",
    "    Accept='application/jsonlines',\n",
    "    Body=modelC_input_data\n",
    "    )\n",
    "outputs = response['Body'].read().decode()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33267ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d13d9",
   "metadata": {},
   "source": [
    "### Local Mode Endpoint Clean-up\n",
    "엔드포인트를 계속 사용하지 않는다면, 엔드포인트를 삭제해야 합니다. SageMaker SDK에서는 delete_endpoint() 메소드로 간단히 삭제할 수 있습니다.\n",
    "참고로, 노트북 인스턴스에서 추론 컨테이너를 배포했기 때문에 엔드포인트를 띄워 놓아도 별도로 추가 요금이 과금되지는 않습니다.\n",
    "\n",
    "로컬 엔드포인트는 도커 컨테이너이기 때문에 `docker rm $(docker ps -a -q)` 으로도 간단히 삭제할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb73be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
